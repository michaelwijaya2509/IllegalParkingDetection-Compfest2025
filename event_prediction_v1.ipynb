{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMConfig:\n",
    "    # Data shape\n",
    "    IMAGE_SIZE = (224, 224)     # resolusi tiap frame\n",
    "    NUM_CLASSES = 2             # binary classification\n",
    "    SEQ_LEN = 16                # jumlah frame per sequence\n",
    "\n",
    "    # Model backbone\n",
    "    MODEL_NAME = \"efficientnet_b7\"\n",
    "    CNN_FEATURE_SIZE = 2560     # output EfficientNet-B7 (n_channels)\n",
    "    LSTM_HIDDEN_SIZE = 512      # ukuran hidden state LSTM\n",
    "    LSTM_NUM_LAYERS = 1\n",
    "    DROPOUT = 0.4\n",
    "\n",
    "    # Training params\n",
    "    BATCH_SIZE = 2              # batch dalam sequence, jadi total frame = BATCH_SIZE*SEQ_LEN\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    PATIENCE = 5\n",
    "    MIN_DELTA = 0.001\n",
    "\n",
    "    # Data augmentation\n",
    "    ROTATION_RANGE = 15\n",
    "    BRIGHTNESS_RANGE = 0.2\n",
    "    CONTRAST_RANGE = 0.2\n",
    "\n",
    "    # Directories\n",
    "    DATA_DIR = \"the_dataset\"\n",
    "    MODEL_DIR = \"models/phase2_cnnlstm\"\n",
    "    RESULTS_DIR = \"results/phase2_cnnlstm\"\n",
    "    LOGS_DIR = \"logs\"\n",
    "\n",
    "    # MLflow\n",
    "    MLFLOW_EXPERIMENT_NAME = \"car_exit_detection_cnnlstm\"\n",
    "    MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.create_directories()\n",
    "        self.setup_mlflow()\n",
    "\n",
    "    def create_directories(self):\n",
    "        directories = [self.MODEL_DIR, self.RESULTS_DIR, self.LOGS_DIR, \"mlruns\"]\n",
    "        for dir_path in directories:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            logger.info(f\"Created directory: {dir_path}\")\n",
    "\n",
    "    def setup_mlflow(self):\n",
    "        mlflow.set_tracking_uri(self.MLFLOW_TRACKING_URI)\n",
    "        try:\n",
    "            mlflow.set_experiment(self.MLFLOW_EXPERIMENT_NAME)\n",
    "            experiment = mlflow.get_experiment_by_name(self.MLFLOW_EXPERIMENT_NAME)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(self.MLFLOW_EXPERIMENT_NAME)\n",
    "                logger.info(f\"Created MLFlow experiment: {self.MLFLOW_EXPERIMENT_NAME}\")\n",
    "            else:\n",
    "                logger.info(f\"Using existing MLFlow experiment: {self.MLFLOW_EXPERIMENT_NAME}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"MLFlow setup failed: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarExitSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Satu sample = 1 sequence berisi T frame (sampling/padding).\n",
    "    Output: (x, y, length) di mana\n",
    "      x: (T, C, H, W)  | y: int label  | length: banyak frame asli (<= T)\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, transform, target_len=16):\n",
    "        self.sequences = sequences\n",
    "        self.transform = transform\n",
    "        self.target_len = target_len\n",
    "\n",
    "    def __len__(self): return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.sequences[idx]\n",
    "        frames = item[\"frame_paths\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Sampling/padding agar panjang = target_len\n",
    "        if len(frames) >= self.target_len:\n",
    "            # uniform sampling sepanjang sequence\n",
    "            sel_idx = np.linspace(0, len(frames)-1, self.target_len).astype(int)\n",
    "            chosen = [frames[i] for i in sel_idx]\n",
    "            length = self.target_len\n",
    "        else:\n",
    "            chosen = frames + [frames[-1]] * (self.target_len - len(frames))\n",
    "            length = len(frames)\n",
    "\n",
    "        imgs = []\n",
    "        for fp in chosen:\n",
    "            img = Image.open(fp).convert('RGB')\n",
    "            imgs.append(self.transform(img))  # transform frame-level (resize/normalize/augment)\n",
    "\n",
    "        x = torch.stack(imgs, dim=0)  # (T, C, H, W)\n",
    "        return x, torch.tensor(label, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "def seq_collate(batch):\n",
    "    xs, ys, ls = zip(*batch)\n",
    "    return torch.stack(xs, 0), torch.stack(ys, 0), torch.stack(ls, 0)  # (B,T,C,H,W), (B,), (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Ekstraktor fitur frame berbasis EfficientNet-B7 (tanpa classifier).\n",
    "    - Jika input (B, C, H, W) -> output (B, D)\n",
    "    - Jika input (B, T, C, H, W) -> output (B, T, D)\n",
    "    D = cnn_feature_size (Eff-B7 = 2560).\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_backbone: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.classifier = nn.Identity()  # keluarkan vektor fitur akhir (B, 2560)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 4:  # (B, C, H, W)\n",
    "            return self.backbone(x)  # (B, D)\n",
    "\n",
    "        elif x.dim() == 5:  # (B, T, C, H, W)\n",
    "            B, T, C, H, W = x.shape\n",
    "            x_flat = x.view(B * T, C, H, W)\n",
    "            feats = self.backbone(x_flat)         # (B*T, D)\n",
    "            D = feats.shape[1]\n",
    "            return feats.view(B, T, D)            # (B, T, D)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceHeadLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Head temporal LSTM untuk fitur per-frame.\n",
    "    Input :\n",
    "      feats   : (B, T, D)  -> fitur dari CNN per frame\n",
    "      lengths : (B,)       -> panjang asli tiap sequence (<= T)\n",
    "    Output:\n",
    "      logits  : (B, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim: int = 2560, hidden: int = 512, layers: int = 1,\n",
    "                 num_classes: int = 2, bidirectional: bool = False, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feat_dim,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=(dropout if layers > 1 else 0.0),\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(out_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        self._init_classifier()\n",
    "\n",
    "    def _init_classifier(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        # feats: (B, T, D), lengths harus di CPU untuk pack\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            feats, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)  # h_n: (num_layers * num_dirs, B, H)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat([h_n[-2], h_n[-1]], dim=1)  # (B, 2H)\n",
    "        else:\n",
    "            h_last = h_n[-1]                               # (B, H)\n",
    "\n",
    "        logits = self.classifier(h_last)                   # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Pembungkus: CNNFeatureExtractor + SequenceHeadLSTM.\n",
    "    Input : x (B,T,C,H,W), lengths (B,)\n",
    "    Output: logits (B,num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, cnn_feature_size=2560,\n",
    "                 lstm_hidden=512, lstm_layers=1, bidirectional=False, dropout=0.3,\n",
    "                 freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.extractor = CNNFeatureExtractor(freeze_backbone=freeze_backbone)\n",
    "        self.head = SequenceHeadLSTM(\n",
    "            feat_dim=cnn_feature_size,\n",
    "            hidden=lstm_hidden,\n",
    "            layers=lstm_layers,\n",
    "            num_classes=num_classes,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x_btchw: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.extractor(x_btchw)            # (B, T, D)\n",
    "        logits = self.head(feats, lengths)         # (B, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import asctime\n",
    "\n",
    "\n",
    "class CNNLSTMTrainer:\n",
    "    def __init__(self, config, device: torch.device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.config.HEAD_LR = getattr(self.config, \"HEAD_LR\", 1e-3)\n",
    "        self.config.SEQ_LEN = getattr(self.config, \"SEQ_LEN\", 16)\n",
    "\n",
    "    def create_transforms(self) -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(self.config.IMAGE_SIZE, scale=(0.85, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.3),\n",
    "            transforms.RandomRotation(degrees=self.config.ROTATION_RANGE),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=self.config.BRIGHTNESS_RANGE,\n",
    "                contrast=self.config.CONTRAST_RANGE,\n",
    "                saturation=0.1,\n",
    "                hue=0.05\n",
    "            ),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)),\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop(self.config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "        return train_transform, val_transform\n",
    "\n",
    "    def _build_head_optimizer(self, model: nn.Module) -> AdamW:\n",
    "        head_params = list(model.head.parameters())\n",
    "        optimizer = AdamW(head_params, lr=self.config.HEAD_LR, weight_decay=self.config.WEIGHT_DECAY)\n",
    "        return optimizer\n",
    "\n",
    "    def train_val_once(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        run_name: str = None\n",
    "    ) -> Tuple[nn.Module, Dict, float]:\n",
    "        \n",
    "        is_active = mlflow.active_run() is not None\n",
    "\n",
    "        with mlflow.start_run(run_name=run_name,nested = is_active):\n",
    "            mlflow.log_params({\n",
    "                'phase': 'Stage-1 (freeze CNN, train LSTM+FC)',\n",
    "                'image_size': str(self.config.IMAGE_SIZE),\n",
    "                'seq_len': self.config.SEQ_LEN,\n",
    "                'batch_size': self.config.BATCH_SIZE,\n",
    "                'epochs': self.config.EPOCHS,\n",
    "                'head_lr': self.config.HEAD_LR,\n",
    "                'weight_decay': self.config.WEIGHT_DECAY,\n",
    "                'dropout': self.config.DROPOUT,\n",
    "                'lstm_hidden': getattr(self.config, \"LSTM_HIDDEN_SIZE\", 512),\n",
    "                'lstm_layers': getattr(self.config, \"LSTM_NUM_LAYERS\", 1),\n",
    "                'backbone': getattr(self.config, \"MODEL_NAME\", \"efficientnet_b7\"),\n",
    "                'freeze_backbone': True\n",
    "            })\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = self._build_head_optimizer(model)\n",
    "            scheduler = OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=self.config.HEAD_LR,\n",
    "                epochs=self.config.EPOCHS,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                pct_start=0.3\n",
    "            )\n",
    "\n",
    "            history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "            best_val_acc, best_state, patience_counter = 0.0, None, 0\n",
    "\n",
    "            for epoch in range(self.config.EPOCHS):\n",
    "                model.train()\n",
    "                train_loss_sum, train_correct, train_total = 0.0, 0, 0\n",
    "                pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.EPOCHS}', leave=False)\n",
    "\n",
    "                for images, labels, lengths in pbar:\n",
    "                    images = images.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    lengths = lengths.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images, lengths)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    bs = labels.size(0)\n",
    "                    train_loss_sum += loss.item() * bs\n",
    "                    _, pred = outputs.max(1)\n",
    "                    train_total += bs\n",
    "                    train_correct += (pred == labels).sum().item()\n",
    "\n",
    "                    pbar.set_postfix(loss=f'{loss.item():.4f}',\n",
    "                                     acc=f'{100.0*train_correct/max(train_total,1):.1f}%')\n",
    "\n",
    "                epoch_train_loss = train_loss_sum / max(train_total, 1)\n",
    "                epoch_train_acc  = train_correct / max(train_total, 1)\n",
    "\n",
    "                model.eval()\n",
    "                val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels, lengths in val_loader:\n",
    "                        images = images.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        lengths = lengths.to(self.device)\n",
    "\n",
    "                        outputs = model(images, lengths)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        bs = labels.size(0)\n",
    "                        val_loss_sum += loss.item() * bs\n",
    "                        _, pred = outputs.max(1)\n",
    "                        val_total += bs\n",
    "                        val_correct += (pred == labels).sum().item()\n",
    "\n",
    "                epoch_val_loss = val_loss_sum / max(val_total, 1)\n",
    "                epoch_val_acc  = val_correct / max(val_total, 1)\n",
    "\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['train_acc'].append(epoch_train_acc)\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "                history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "                mlflow.log_metrics({\n",
    "                    'train_loss': epoch_train_loss,\n",
    "                    'train_acc': epoch_train_acc,\n",
    "                    'val_loss': epoch_val_loss,\n",
    "                    'val_acc': epoch_val_acc\n",
    "                }, step=epoch)\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch+1}/{self.config.EPOCHS} \"\n",
    "                    f\"Train: {epoch_train_loss:.4f}/{epoch_train_acc:.4f} \"\n",
    "                    f\"Val: {epoch_val_loss:.4f}/{epoch_val_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "                if epoch_val_acc > best_val_acc + self.config.MIN_DELTA:\n",
    "                    best_val_acc = epoch_val_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    patience_counter = 0\n",
    "                    logger.info(f\"New best model — Val Acc: {epoch_val_acc:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.config.PATIENCE:\n",
    "                        logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            if best_state is not None:\n",
    "                model.load_state_dict(best_state)\n",
    "\n",
    "            os.makedirs(self.config.MODEL_DIR, exist_ok=True)\n",
    "            best_model_path = os.path.join(self.config.MODEL_DIR, 'best_cnnlstm_stage1.pth')\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "                        'best_val_acc': best_val_acc,\n",
    "                        'config': self.config.__dict__}, best_model_path)\n",
    "            mlflow.log_artifact(best_model_path, \"models\")\n",
    "\n",
    "            return model, history, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence_index(data_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Return list of sequence dict:\n",
    "      [\n",
    "        {\"sequence_id\": \"0_seq001\", \"frame_paths\": [...], \"label\": 0},\n",
    "        {\"sequence_id\": \"1_seq123\", \"frame_paths\": [...], \"label\": 1},\n",
    "        ...\n",
    "      ]\n",
    "    Struktur diasumsikan: data_dir/{0,1}/{seq_name}/*.jpg|png|jpeg|bmp\n",
    "    \"\"\"\n",
    "    root = Path(data_dir)\n",
    "    exts = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    sequences: List[Dict] = []\n",
    "\n",
    "    for cls in [0, 1]:\n",
    "        class_dir = root / str(cls)\n",
    "        if not class_dir.exists():\n",
    "            logger.warning(f\"Class directory {class_dir} not found\")\n",
    "            continue\n",
    "\n",
    "        for seq_dir in sorted([p for p in class_dir.iterdir() if p.is_dir()]):\n",
    "            frames = sorted([f for f in seq_dir.iterdir()\n",
    "                             if f.is_file() and f.suffix.lower() in exts])\n",
    "            if not frames:\n",
    "                logger.warning(f\"No images in sequence {seq_dir}\")\n",
    "                continue\n",
    "\n",
    "            sequences.append({\n",
    "                \"sequence_id\": f\"{cls}_{seq_dir.name}\",\n",
    "                \"frame_paths\": [str(x) for x in frames],\n",
    "                \"label\": int(cls),\n",
    "            })\n",
    "\n",
    "    if not sequences:\n",
    "        raise ValueError(\"No sequences found.\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(sequences)} sequences \"\n",
    "                f\"({sum(len(s['frame_paths']) for s in sequences)} frames total)\")\n",
    "    return sequences\n",
    "\n",
    "def split_train_val_sequences(sequences, val_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Split sekali anti-leak: semua frame dari 1 sequence jatuh ke train ATAU val saja.\n",
    "    \"\"\"\n",
    "    groups = [s[\"sequence_id\"] for s in sequences]\n",
    "    labels = [s[\"label\"] for s in sequences]\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(sequences, labels, groups))\n",
    "\n",
    "    train_seq = [sequences[i] for i in train_idx]\n",
    "    val_seq   = [sequences[i] for i in val_idx]\n",
    "\n",
    "    logger.info(f\"Train sequences: {len(train_seq)} | Val sequences: {len(val_seq)}\")\n",
    "    return train_seq, val_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trainval_history(history: Dict, config, title: str = \"CNN+LSTM Stage-1 (Freeze CNN)\",\n",
    "                               best_val_acc: float = None, filename: str = \"trainval_history_stage1.png\"):\n",
    "    \"\"\"\n",
    "    history: dict dengan keys:\n",
    "        - 'train_acc', 'val_acc', 'train_loss', 'val_loss'  (list per-epoch)\n",
    "    config : punya RESULTS_DIR untuk lokasi simpan\n",
    "    \"\"\"\n",
    "    train_acc = np.array(history.get('train_acc', []), dtype=float)\n",
    "    val_acc   = np.array(history.get('val_acc', []), dtype=float)\n",
    "    train_ls  = np.array(history.get('train_loss', []), dtype=float)\n",
    "    val_ls    = np.array(history.get('val_loss', []), dtype=float)\n",
    "\n",
    "    epochs = np.arange(1, len(train_acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Acc panel\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.plot(epochs, train_acc, label='Train Acc', linewidth=2)\n",
    "    ax1.plot(epochs, val_acc,   label='Val Acc',   linewidth=2)\n",
    "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy per Epoch')\n",
    "    ax1.grid(True, alpha=0.3); ax1.legend()\n",
    "\n",
    "    # Loss panel\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.plot(epochs, train_ls, label='Train Loss', linewidth=2)\n",
    "    ax2.plot(epochs, val_ls,   label='Val Loss',   linewidth=2)\n",
    "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Loss per Epoch')\n",
    "    ax2.grid(True, alpha=0.3); ax2.legend()\n",
    "\n",
    "    suptitle = title if best_val_acc is None else f\"{title}  |  Best Val Acc: {best_val_acc:.4f}\"\n",
    "    plt.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(config.RESULTS_DIR, filename)\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(out_path)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set_sequence(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    config\n",
    ") -> dict:\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, lengths in tqdm(test_loader, desc=\"Evaluating test set\"):\n",
    "            images  = images.to(device)    # (B,T,C,H,W)\n",
    "            labels  = labels.to(device)    # (B,)\n",
    "            lengths = lengths.to(device)   # (B,)\n",
    "\n",
    "            outputs = model(images, lengths)          # (B,num_classes)\n",
    "            _, pred = torch.max(outputs.data, dim=1)  # (B,)\n",
    "\n",
    "            test_total   += labels.size(0)\n",
    "            test_correct += (pred == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(pred.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # -- Metrics (dengan zero_division=0 agar aman kalau ada kelas kosong)\n",
    "    accuracy   = test_correct / max(test_total, 1)\n",
    "    f1_w       = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    prec_w     = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    rec_w      = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    f1_per     = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    prec_per   = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    rec_per    = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1_w,\n",
    "        'precision_weighted': prec_w,\n",
    "        'recall_weighted': rec_w,\n",
    "        'f1_normal': f1_per[0] if len(f1_per) > 0 else 0.0,\n",
    "        'f1_person_exiting': f1_per[1] if len(f1_per) > 1 else 0.0,\n",
    "        'precision_normal': prec_per[0] if len(prec_per) > 0 else 0.0,\n",
    "        'precision_person_exiting': prec_per[1] if len(prec_per) > 1 else 0.0,\n",
    "        'recall_normal': rec_per[0] if len(rec_per) > 0 else 0.0,\n",
    "        'recall_person_exiting': rec_per[1] if len(rec_per) > 1 else 0.0,\n",
    "        'test_samples': test_total\n",
    "    }\n",
    "\n",
    "    # -- Confusion Matrix\n",
    "    class_names = ['Normal Parking', 'Person Exiting']\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Test Set Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "\n",
    "    os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "    cm_path = os.path.join(config.RESULTS_DIR, 'test_confusion_matrix_seq.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "    mlflow.log_artifact(cm_path)\n",
    "\n",
    "    # -- Classification report (log ke logger)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "    logger.info(\"\\nTEST SET EVALUATION (SEQUENCE):\")\n",
    "    logger.info(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    logger.info(f\"F1 (weighted): {f1_w:.4f} | Precision (weighted): {prec_w:.4f} | Recall (weighted): {rec_w:.4f}\")\n",
    "    logger.info(f\"\\nDetailed Classification Report:\\n{report}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # ---------- setup ----------\n",
    "        set_seed(42)\n",
    "        config = CNNLSTMConfig()  # ganti ke config sequence kamu (bukan CNNConfig lama)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Device: {device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            logger.info(f\"GPU Memory: {mem_gb:.1f} GB\")\n",
    "\n",
    "        # ---------- load sequences ----------\n",
    "        sequences = load_sequence_index(config.DATA_DIR)  # -> list of dicts (sequence-level)\n",
    "        class_counts = {\n",
    "            0: sum(1 for s in sequences if s[\"label\"] == 0),\n",
    "            1: sum(1 for s in sequences if s[\"label\"] == 1),\n",
    "        }\n",
    "        logger.info(f\"Sequence count: {len(sequences)} | Class dist (seq): {class_counts}\")\n",
    "\n",
    "        # sekali split train/val tanpa test (anti-leak per sequence)\n",
    "        train_seq, val_seq = split_train_val_sequences(sequences, val_size=0.2, seed=42)\n",
    "        logger.info(f\"Train sequences: {len(train_seq)} | Val sequences: {len(val_seq)}\")\n",
    "\n",
    "        # ---------- transforms ----------\n",
    "        trainer = CNNLSTMTrainer(config, device)\n",
    "        train_tf, val_tf = trainer.create_transforms()\n",
    "\n",
    "        # ---------- datasets & loaders ----------\n",
    "        train_ds = CarExitSequenceDataset(train_seq, transform=train_tf, target_len=config.SEQ_LEN)\n",
    "        val_ds   = CarExitSequenceDataset(val_seq,   transform=val_tf,   target_len=config.SEQ_LEN)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            collate_fn=seq_collate\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            collate_fn=seq_collate\n",
    "        )\n",
    "\n",
    "        # ---------- model (Stage-1: freeze backbone) ----------\n",
    "        model = CNNLSTMModel(\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            cnn_feature_size=config.CNN_FEATURE_SIZE,   # 2560 for EffNet-B7\n",
    "            lstm_hidden=config.LSTM_HIDDEN_SIZE,\n",
    "            lstm_layers=config.LSTM_NUM_LAYERS,\n",
    "            bidirectional=False,\n",
    "            dropout=config.DROPOUT,\n",
    "            freeze_backbone=True,                       # penting: bekukan CNN\n",
    "        ).to(device)\n",
    "\n",
    "        # sanity log: jumlah param trainable vs frozen\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        frozen    = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        logger.info(f\"Trainable params: {trainable:,} | Frozen params: {frozen:,}\")\n",
    "\n",
    "        # ---------- train once (no K-Fold) ----------\n",
    "        run_name = f\"cnn_lstm_stage1_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        best_model, history, best_val_acc = trainer.train_val_once(model, train_loader, val_loader, run_name=run_name)\n",
    "\n",
    "        # ---------- visualize ----------\n",
    "        plot_path = visualize_trainval_history(\n",
    "            history, config,\n",
    "            title=\"CNN+LSTM Stage-1 (Freeze CNN)\",\n",
    "            best_val_acc=best_val_acc,\n",
    "            filename=\"trainval_history_stage1.png\"\n",
    "        )\n",
    "        logger.info(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "        # ---------- save summary ----------\n",
    "        results_summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'phase': 'Stage 1 - CNN+LSTM (freeze CNN, train LSTM+FC)',\n",
    "            'model_architecture': 'EfficientNet-B7 (frozen) + LSTM + FC',\n",
    "            'config': {\n",
    "                'model_name': config.MODEL_NAME,\n",
    "                'image_size': config.IMAGE_SIZE,\n",
    "                'seq_len': config.SEQ_LEN,\n",
    "                'batch_size': config.BATCH_SIZE,\n",
    "                'epochs': config.EPOCHS,\n",
    "                'head_lr': getattr(config, \"HEAD_LR\", 1e-3),\n",
    "                'weight_decay': config.WEIGHT_DECAY,\n",
    "                'dropout': config.DROPOUT,\n",
    "                'lstm_hidden': config.LSTM_HIDDEN_SIZE,\n",
    "                'lstm_layers': config.LSTM_NUM_LAYERS,\n",
    "                'freeze_backbone': True\n",
    "            },\n",
    "            'data_info': {\n",
    "                'total_sequences': len(sequences),\n",
    "                'train_sequences': len(train_seq),\n",
    "                'val_sequences': len(val_seq),\n",
    "                'class_distribution_sequences': class_counts\n",
    "            },\n",
    "            'final_val': {\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'epochs_ran': len(history.get('val_acc', []))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "        results_path = os.path.join(config.RESULTS_DIR, 'stage1_cnnlstm_results.json')\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2)\n",
    "        mlflow.log_artifact(results_path)\n",
    "\n",
    "        logger.info(f\"Best Val Accuracy: {best_val_acc:.4f}\")\n",
    "        logger.info(\"\\nSAVED FILES:\")\n",
    "        logger.info(f\"   Best model: {config.MODEL_DIR}/best_cnnlstm_stage1.pth\")\n",
    "        logger.info(f\"   Results summary: {results_path}\")\n",
    "        logger.info(f\"   Visualizations: {config.RESULTS_DIR}/\")\n",
    "        logger.info(f\"   MLFlow tracking: {config.MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "        return best_model, history, results_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training pipeline failed: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RAW DATA SUMMARY ================\n",
      "Root           : C:\\Users\\andre\\Documents\\Binus\\Lomba\\compfest\\IllegalParkingDetection-Compfest2025\\the_dataset\n",
      "Total sequences: 18\n",
      "  Label 0      : 5 sequences\n",
      "  Label 1      : 13 sequences\n",
      "\n",
      "Sequence length stats (jumlah frame per sequence):\n",
      "  Min/25%/Med/75%/Max : 16 / 16 / 16 / 16 / 26\n",
      "  Mean ± Std          : 18.28 ± 4.13\n",
      "  Contoh histogram (bucket 5‑frame):\n",
      "     15– 19 : 14\n",
      "     25– 29 : 4\n",
      "\n",
      "[OK] Sampel file gambar valid (cek cepat)\n",
      "\n",
      "Contoh sequence (preview path):\n",
      "  0_event_v020_p028_180 | label=0 | frames=16\n",
      "    first 3 : ['the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_000.jpg', 'the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_001.jpg', 'the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_002.jpg']\n",
      "    last  3 : ['the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_013.jpg', 'the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_014.jpg', 'the_dataset\\\\0\\\\event_v020_p028_180\\\\frame_015.jpg']\n",
      "  0_event_v2247_p2249_24186 | label=0 | frames=16\n",
      "    first 3 : ['the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_000.jpg', 'the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_001.jpg', 'the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_002.jpg']\n",
      "    last  3 : ['the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_013.jpg', 'the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_014.jpg', 'the_dataset\\\\0\\\\event_v2247_p2249_24186\\\\frame_015.jpg']\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset_root(data_dir: str, max_corrupt_check_per_seq: int = 3):\n",
    "    \"\"\"\n",
    "    Struktur asumsi:\n",
    "      data_dir/\n",
    "        0/\n",
    "          seq_001/ frame_0001.jpg ...\n",
    "          seq_002/ ...\n",
    "        1/\n",
    "          seq_101/ ...\n",
    "    Output:\n",
    "      - sequences: list[dict] = [{\"sequence_id\", \"label\", \"frame_paths\"}]\n",
    "      - prints: ringkasan super detail soal isi dataset SEBELUM proses apa pun\n",
    "    \"\"\"\n",
    "    root = Path(data_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"DATA_DIR nggak ada: {root}\")\n",
    "\n",
    "    sequences = []\n",
    "    empty_sequences = []\n",
    "    non_image_files = []\n",
    "    exts_ok = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "    # 1) Scan folder per label\n",
    "    for label in [0, 1]:\n",
    "        class_dir = root / str(label)\n",
    "        if not class_dir.exists():\n",
    "            print(f\"[WARN] Folder kelas tidak ditemukan: {class_dir}\")\n",
    "            continue\n",
    "        seq_dirs = sorted([p for p in class_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "        for sd in seq_dirs:\n",
    "            # kumpulkan file gambar valid (by extension)\n",
    "            all_files = sorted([p for p in sd.iterdir() if p.is_file()])\n",
    "            imgs = [p for p in all_files if p.suffix.lower() in exts_ok]\n",
    "            others = [p for p in all_files if p.suffix.lower() not in exts_ok]\n",
    "            non_image_files.extend(others)\n",
    "\n",
    "            if len(imgs) == 0:\n",
    "                empty_sequences.append(sd)\n",
    "                continue\n",
    "\n",
    "            sequences.append({\n",
    "                \"sequence_id\": f\"{label}_{sd.name}\",\n",
    "                \"label\": label,\n",
    "                \"frame_paths\": [str(p) for p in imgs],  # simpan string path\n",
    "            })\n",
    "\n",
    "    # 2) Ringkasan dasar\n",
    "    n_seq = len(sequences)\n",
    "    by_label = Counter([s[\"label\"] for s in sequences])\n",
    "\n",
    "    print(\"\\n================ RAW DATA SUMMARY ================\")\n",
    "    print(f\"Root           : {root.resolve()}\")\n",
    "    print(f\"Total sequences: {n_seq}\")\n",
    "    for k in sorted(by_label.keys()):\n",
    "        print(f\"  Label {k}      : {by_label[k]} sequences\")\n",
    "\n",
    "    if empty_sequences:\n",
    "        print(f\"\\n[WARN] Empty sequences (tanpa frame): {len(empty_sequences)}\")\n",
    "        for p in empty_sequences[:10]:\n",
    "            print(f\"   - {p}\")\n",
    "        if len(empty_sequences) > 10:\n",
    "            print(f\"   ... ({len(empty_sequences)-10} lagi)\")\n",
    "\n",
    "    if non_image_files:\n",
    "        print(f\"\\n[INFO] Non-image files terdeteksi: {len(non_image_files)}\")\n",
    "        for p in non_image_files[:10]:\n",
    "            print(f\"   - {p.name}\")\n",
    "        if len(non_image_files) > 10:\n",
    "            print(f\"   ... ({len(non_image_files)-10} lagi)\")\n",
    "\n",
    "    # 3) Statistik panjang sequence (jumlah frame)\n",
    "    lens = np.array([len(s[\"frame_paths\"]) for s in sequences], dtype=int)\n",
    "    if len(lens) > 0:\n",
    "        q = np.quantile(lens, [0, 0.25, 0.5, 0.75, 1.0])\n",
    "        print(\"\\nSequence length stats (jumlah frame per sequence):\")\n",
    "        print(f\"  Min/25%/Med/75%/Max : {int(q[0])} / {int(q[1])} / {int(q[2])} / {int(q[3])} / {int(q[4])}\")\n",
    "        print(f\"  Mean ± Std          : {lens.mean():.2f} ± {lens.std():.2f}\")\n",
    "\n",
    "        # histogram sederhana\n",
    "        hist = defaultdict(int)\n",
    "        for L in lens:\n",
    "            # bucketin per 5 frame biar kebaca\n",
    "            bucket = int(L // 5) * 5\n",
    "            hist[bucket] += 1\n",
    "        top = sorted(hist.items())[:10]\n",
    "        print(\"  Contoh histogram (bucket 5‑frame):\")\n",
    "        for b, c in top:\n",
    "            print(f\"    {b:>3d}–{b+4:>3d} : {c}\")\n",
    "\n",
    "    # 4) Cek cepat korup/ga kebaca (sample beberapa frame per sequence)\n",
    "    #    Tujuan: sebelum training, kita tahu ada file rusak atau tidak.\n",
    "    bad_images = []\n",
    "    for s in sequences[: min(len(sequences), 50)]:  # batasi 50 seq biar cepat\n",
    "        for p in s[\"frame_paths\"][:max_corrupt_check_per_seq]:\n",
    "            try:\n",
    "                with Image.open(p) as im:\n",
    "                    im.verify()  # cepat, cek header\n",
    "            except Exception as e:\n",
    "                bad_images.append((p, repr(e)))\n",
    "    if bad_images:\n",
    "        print(f\"\\n[ERROR] Ditemukan file gambar korup/tak bisa dibaca: {len(bad_images)}\")\n",
    "        for p, err in bad_images[:10]:\n",
    "            print(f\"   - {p} :: {err}\")\n",
    "        if len(bad_images) > 10:\n",
    "            print(f\"   ... ({len(bad_images)-10} lagi)\")\n",
    "    else:\n",
    "        print(\"\\n[OK] Sampel file gambar valid (cek cepat)\")\n",
    "\n",
    "    # 5) Tampilkan contoh 1–2 sequence (path frame awal/akhir)\n",
    "    print(\"\\nContoh sequence (preview path):\")\n",
    "    for s in sequences[:2]:\n",
    "        fps = s[\"frame_paths\"]\n",
    "        print(f\"  {s['sequence_id']} | label={s['label']} | frames={len(fps)}\")\n",
    "        print(f\"    first 3 : {fps[:3]}\")\n",
    "        print(f\"    last  3 : {fps[-3:]}\")\n",
    "\n",
    "    print(\"==================================================\\n\")\n",
    "    return sequences\n",
    "\n",
    "# === PANGGIL DI SINI ===\n",
    "DATA_DIR = \"the_dataset\"  # ganti sesuai folder dataset kamu\n",
    "sequences = inspect_dataset_root(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_val_transform(config):\n",
    "    from torchvision import transforms\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "def _load_sequence_frames(seq_dir, transform, target_len):\n",
    "    # kumpulkan file gambar, sort biar urut waktu\n",
    "    exts = ('*.jpg','*.jpeg','*.png','*.bmp')\n",
    "    paths = []\n",
    "    for e in exts:\n",
    "        paths.extend(glob.glob(os.path.join(seq_dir, e)))\n",
    "    paths = sorted(paths)\n",
    "    if len(paths) == 0:\n",
    "        raise FileNotFoundError(f\"Tidak ada frame di folder: {seq_dir}\")\n",
    "\n",
    "    # sampling/padding ke target_len\n",
    "    if len(paths) >= target_len:\n",
    "        idx = torch.linspace(0, len(paths)-1, steps=target_len).round().long().tolist()\n",
    "        sel = [paths[i] for i in idx]\n",
    "        length = target_len\n",
    "    else:\n",
    "        sel = paths[:]\n",
    "        # pad dengan frame terakhir\n",
    "        sel += [paths[-1]] * (target_len - len(paths))\n",
    "        length = len(paths)\n",
    "\n",
    "    # load & transform -> (T,C,H,W)\n",
    "    frames = []\n",
    "    for p in sel:\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        frames.append(transform(img))\n",
    "    x_tchw = torch.stack(frames, dim=0)  # (T,C,H,W)\n",
    "    return x_tchw, length, sel\n",
    "\n",
    "def run_prediction_test(seq_dir: str):\n",
    "    # gunakan config untuk CNN+LSTM (bukan CNNConfig lama)\n",
    "    config = CNNLSTMConfig()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # siapkan model (harus sama dengan yang dipakai saat training)\n",
    "    model = CNNLSTMModel(\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        cnn_feature_size=config.CNN_FEATURE_SIZE,\n",
    "        lstm_hidden=config.LSTM_HIDDEN_SIZE,\n",
    "        lstm_layers=config.LSTM_NUM_LAYERS,\n",
    "        bidirectional=False,\n",
    "        dropout=config.DROPOUT,\n",
    "        freeze_backbone=True,  # stage-1\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load checkpoint stage-1\n",
    "    ckpt_path = os.path.join(config.MODEL_DIR, 'best_cnnlstm_stage1.pth')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint tidak ditemukan: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "    # siapkan sequence tensor\n",
    "    val_tf = _build_val_transform(config)\n",
    "    x_tchw, length, used_paths = _load_sequence_frames(seq_dir, val_tf, config.SEQ_LEN)\n",
    "\n",
    "    # ke batch: (1,T,C,H,W), lengths: (1,)\n",
    "    x_btchw = x_tchw.unsqueeze(0).to(device)\n",
    "    lengths = torch.tensor([length], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_btchw, lengths)        # (1,num_classes)\n",
    "        probs  = F.softmax(logits, dim=1)[0]    # (num_classes,)\n",
    "        pred   = torch.argmax(probs).item()\n",
    "\n",
    "    class_map = {0: 'normal_parking', 1: 'person_exiting'}\n",
    "    result = {\n",
    "        'prediction': pred,\n",
    "        'class_name': class_map.get(pred, str(pred)),\n",
    "        'probabilities': {\n",
    "            'normal_parking': float(probs[0].item()),\n",
    "            'person_exiting': float(probs[1].item())\n",
    "        },\n",
    "        'seq_dir': seq_dir,\n",
    "        'used_frames': used_paths,\n",
    "        'effective_length': int(length),\n",
    "        'target_len': int(config.SEQ_LEN),\n",
    "    }\n",
    "\n",
    "    print(f\"Prediction: {result['class_name']}\")\n",
    "    print(f\"  normal_parking  : {result['probabilities']['normal_parking']:.4f}\")\n",
    "    print(f\"  person_exiting  : {result['probabilities']['person_exiting']:.4f}\")\n",
    "    print(f\"Frames used: {result['effective_length']}/{result['target_len']} • {seq_dir}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a730c3b96bc4d6e884723d66d86eb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     best_model, history, results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# ganti path ini dengan folder sequence yang mau kamu tes\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         seq_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe_dataset/1/seq_101\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# <-- contoh\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 75\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# ---------- train once (no K-Fold) ----------\u001b[39;00m\n\u001b[0;32m     74\u001b[0m run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_lstm_stage1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 75\u001b[0m best_model, history, best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_val_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# ---------- visualize ----------\u001b[39;00m\n\u001b[0;32m     78\u001b[0m plot_path \u001b[38;5;241m=\u001b[39m visualize_trainval_history(\n\u001b[0;32m     79\u001b[0m     history, config,\n\u001b[0;32m     80\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN+LSTM Stage-1 (Freeze CNN)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     best_val_acc\u001b[38;5;241m=\u001b[39mbest_val_acc,\n\u001b[0;32m     82\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainval_history_stage1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m )\n",
      "Cell \u001b[1;32mIn[67], line 86\u001b[0m, in \u001b[0;36mCNNLSTMTrainer.train_val_once\u001b[1;34m(self, model, train_loader, val_loader, run_name)\u001b[0m\n\u001b[0;32m     83\u001b[0m train_loss_sum, train_correct, train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     84\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels, lengths \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     87\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     88\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[54], line 32\u001b[0m, in \u001b[0;36mCarExitSequenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m chosen:\n\u001b[0;32m     31\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(fp)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     imgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# transform frame-level (resize/normalize/augment)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(imgs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (T, C, H, W)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), torch\u001b[38;5;241m.\u001b[39mtensor(length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\datsci\\lib\\site-packages\\PIL\\Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2320\u001b[0m         )\n\u001b[0;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2326\u001b[0m         )\n\u001b[1;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    best_model, history, results = main()\n",
    "\n",
    "    if best_model is not None:\n",
    "        # ganti path ini dengan folder sequence yang mau kamu tes\n",
    "        seq_dir = \"the_dataset/1/seq_101\"   # <-- contoh\n",
    "        try:\n",
    "            _ = run_prediction_test(seq_dir=seq_dir)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction test failed: {e}\")\n",
    "    else:\n",
    "        logger.error(\"\\nStage 1 CNN+LSTM implementation failed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
